Metabase Pulse: Created metabase pulse/alert and mail for all the audits submissions entries of the day in the DB so that AOM’s can verify whether their audits are submitted in the BE. Resolved the issue where AOM’s claims to have submitted audit and didn’t got captured in DB. (Own initiative)

Redis Keys Clear Job: Created redis Jenkins jobs which provides the list of keys and can clear the keys through it with such a logic which does not allow anyone to clear all the keys through this job (User must enter a key and sleect appropriate service to clear the keys). Removed the dependency to get the redis cleared via ssh. (Own initiative)

Cron for jenkins job failure: Created Jenkins last build failure alert. Resolved the issue coming in multiple important prod crons which were failing and removed muliple jobs and crons which were nopt required now. (Own initiative)

SSL certificate expire cron: SSL certificate expiration alert.

Uchiwa Dashboard: Cleaned up uchiwa dashboard.

Invoice Ingestion Automation: Created automation for invoice ingestion to save bandwidth of SRE’s as sometimes their is request for 1000+ invoices to be ingested. (Own initiative)

Cron for Refund entry missing: Created cron to raise alert for booking having entry in refund DB but missing entry in CRS db. (Own initiative)

AWS DMS Service: Explored the DMS service of AWS in detail to migrate the old produciton env databases to the new facets env prodcution databases and succesfully completed the data migration to facets env including ongoing live data replication.

Automation of DMS: Automated the entire DMS task from old env to new env so that manual efforts can be reduced and human error can be avoided as their were 14 servcies were DMS was initiated. (Save upto 2 hrs of DMS task initialization via automation).

Automation of Reverse DMS Sync: Initiated and automated the DMS reverse sync also so that after new env goes live and things goes out of control due to some unexpected errors then swithing back to old env will only require DNS switch and env will work as expected as old env databases are already populated with the new data.

Script for Schema DIff: Created schema diff script which played a cruicial role in the data migration as there were no available realiable source which was useful to give us schema diff in all the tables of 14 databases. Script gave us all the indexes diff, structure diff and the rows count also.

PSQL Function to truncate all the tables: Created psql function/script which can easily tuncate all the tables in the particular database corresponding to the specified schema.

PSQL Function to update all the sequences: Created psql fucntion/script which can easily udpate all the sequences in the particular database corresponding to the specified schema.

Automation of S3 migration with maching pattern: Automated a realiable scripts for the S3 files migration to the new cluster and tenant specific sub folders which copied the files from old structure folders to the new respective sub folders (6 types of files were migrated) and then their file URL in the respective database tables entries were also updated to the new path and ceated the logic in such a way that a entry which already got updated once will not get updated twice, converted the script into a jenkins jobs.

SoanarQube Implemetation: Implemented SonarQube in all the repos of organisation in a efficient automation way.

Cluster Scale Down: Created a reliable script aka one click job to pause and start the staging cluster(cost optimization)

ClamAV script: Script to Stop clamAV service accross all the produciton machines.

Data Archieval: 

CI Codepipelines Creation:

Deploying New services on prod and stag:

Terrform code changes for IAM external ID implementation.

Scrut Danger Alerts resolved.

Terraform: implementing on other accounts.

Dowtime: Monitoring resouces consumption to reduce application downtimes.

https://www.hashicorp.com/certification/terraform-associate				V.Important


terraform workspaces
serverless.tf